# üï∏Ô∏è **Projeto de Web Scraping**

Este projeto demonstra como extrair automaticamente cita√ß√µes do site http://quotes.toscrape.com utilizando **Python**, `requests` e `BeautifulSoup`. Ao final, os dados coletados s√£o organizados em um DataFrame do `pandas` e exportados para um arquivo CSV.

---

## üéØ **Objetivo**
- Desenvolver um script simples e replic√°vel para **raspar** cita√ß√µes (texto e autor) de uma p√°gina web est√°tica.  
- Demonstrar etapas de **coleta**, **parsing** e **manipula√ß√£o** de dados em Python, culminando na gera√ß√£o de um arquivo CSV com as informa√ß√µes.

---

## üõ† **Tecnologias e Ferramentas Usadas**
- **Linguagem:** Python  
- **Bibliotecas:**
  - `requests`: Para enviar requisi√ß√µes HTTP e obter o HTML bruto da p√°gina.  
  - `BeautifulSoup` (do pacote `bs4`): Para analisar o DOM e extrair elementos desejados.  
  - `pandas`: Para estruturar e manipular os dados coletados em um DataFrame e salv√°-los em CSV.  
  - `Jupyter Notebook`: Ambiente interativo para desenvolvimento e demonstra√ß√£o do fluxo de trabalho.  
  - `Conda` ou `venv`: Gerenciamento do ambiente de depend√™ncias.

---

## üìÇ **Estrutura do Projeto**
### **Arquivos e Diret√≥rios**
- **`webscraping_example.ipynb`**: Notebook principal contendo todas as etapas de scraping, an√°lise e exporta√ß√£o.  
- **`quotes.csv`**: Arquivo CSV gerado contendo as cita√ß√µes extra√≠das (texto, autor e tamanho).  
- **`README.md`**: Descri√ß√£o completa do projeto, instru√ß√µes e metodologia.

```
    .
    ‚îú‚îÄ‚îÄ webscraping_example.ipynb
    ‚îú‚îÄ‚îÄ quotes.csv
    ‚îî‚îÄ‚îÄ README.md
```
---

## üß† **M√©todos Implementados**
- **Importa√ß√£o de Bibliotecas**:  
  - In√≠cio do notebook com `import requests`, `from bs4 import BeautifulSoup` e `import pandas as pd` para preparar o ambiente de scraping e an√°lise.

- **Requisi√ß√£o HTTP e Valida√ß√£o**:  
  - Uso de:
    ```python
    url = 'http://quotes.toscrape.com'
    response = requests.get(url)
    ```
  - Verifica√ß√£o de `response.status_code == 200` para garantir que a p√°gina foi acessada corretamente.

- **Parsing do HTML com BeautifulSoup**:  
  - Cria√ß√£o de:
    ```python
    soup = BeautifulSoup(response.text, 'html.parser')
    ```
    para construir a √°rvore DOM.  
  - Sele√ß√£o de todos os elementos `<div class="quote">` por meio de:
    ```python
    quotes = soup.find_all('div', class_='quote')
    ```
    ou
    ```python
    quotes = soup.select('div.quote')
    ```

- **Extra√ß√£o dos Dados (Texto e Autor)**:  
  - Itera√ß√£o sobre cada bloco de cita√ß√£o extra√≠do, usando:
    ```python
    data = []
    for quote in quotes:
        text = quote.find('span', class_='text').get_text()
        author = quote.find('small', class_='author').get_text()
        data.append({'quote': text, 'author': author})
    ```
  - Armazenamento em lista de dicion√°rios, seguida de convers√£o em DataFrame via:
    ```python
    df = pd.DataFrame(data)
    ```

- **Manipula√ß√£o e Explora√ß√£o R√°pida com pandas**:  
  - Exibi√ß√£o das primeiras linhas:
    ```python
    df.head()
    ```
  - Verifica√ß√£o de estrutura e tipos:
    ```python
    df.info()
    ```
  - Contagem de cita√ß√µes por autor:
    ```python
    df['author'].value_counts()
    ```
  - Filtragem por palavra-chave, por exemplo:
    ```python
    df[df['quote'].str.contains('truth', case=False)]
    ```
  - Adi√ß√£o de coluna `length` com o tamanho de cada cita√ß√£o:
    ```python
    df['length'] = df['quote'].str.len()
    df.describe()
    ```
  - Ordena√ß√£o por tamanho de cita√ß√£o:
    ```python
    df.sort_values(by='length', ascending=False)
    ```

- **Exporta√ß√£o para CSV**:  
  - Salvamento completo em:
    ```python
    df.to_csv('quotes.csv', index=False)
    ```

---

## üìä **Resultados Obtidos**
Ap√≥s a execu√ß√£o do notebook, foram coletadas todas as cita√ß√µes exibidas na p√°gina inicial de http://quotes.toscrape.com (totalizando 10 registros). Cada registro cont√©m as colunas **quote** (texto da frase), **author** (nome do autor) e **length** (n√∫mero de caracteres do texto).  

**Conclus√£o Geral**: O script demonstrou capacidade de coletar e estruturar dados de forma r√°pida, al√©m de permitir manipula√ß√£o b√°sica dos resultados com pandas para an√°lises simples de frequ√™ncia e tamanho das cita√ß√µes.



## üöÄ **Pr√≥ximos Passos**
- **Pagina√ß√£o Autom√°tica**: Implementar l√≥gica de itera√ß√£o por m√∫ltiplas p√°ginas verificando o link ‚Äúnext‚Äù.  
- **Tratamento de Erros e Retries**: Adicionar blocos `try/except` e estrat√©gia de backoff para lidar com falhas de conex√£o ou status diferentes de 200.  
- **Rota√ß√£o de User-Agent e Uso de Proxies**: Incluir cabe√ßalhos din√¢micos e proxies para evitar bloqueios em sites mais restritivos.  
- **Extra√ß√£o de Conte√∫do Din√¢mico**: Introduzir Selenium ou Playwright para capturar dados renderizados por JavaScript.  
- **Armazenamento em Banco de Dados**: Em vez de CSV, persistir os resultados em SQLite, PostgreSQL ou outro banco, usando `df.to_sql()`.

---

## üë§ **Autor**
Guilherme Koiti Tanaka Sassaki  
[LinkedIn](https://www.linkedin.com/in/guilherme-sassaki-10b81ba7/)

